{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the final project of Deep Learning for Robot Perception, University of Michigan. This project proposes a perception system involving object detection and depth estimation to localize cutting points for grape bunches. The idea is to come up with a vision system capable of allowing robots to perceive grape stems and motion plan to grasp them.\n",
    "\n",
    "Run some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google drive stuff, ignore for local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\", True)\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# path = '/content/drive/My Drive/grape_net'\n",
    "# sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello grape_net!\n",
      "hello helpers!\n"
     ]
    }
   ],
   "source": [
    "from grape_net import hello_grape_net\n",
    "from helpers import hello_helpers\n",
    "\n",
    "hello_grape_net()\n",
    "hello_helpers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 images from folder grape_stem_data/grape_dataset/img\n",
      "Loaded 20 labels from folder grape_stem_data/grape_dataset/ann\n",
      "Dataset loaded: 20 images and labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from grape_net import GrapeNet\n",
    "from grape_net import GrapeDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#DATASET HERE\n",
    "img_folder = 'grape_stem_data/grape_dataset/img'\n",
    "label_folder = 'grape_stem_data/grape_dataset/ann'\n",
    "grape_dataset = GrapeDataset(img_folder, label_folder)\n",
    "\n",
    "print(f'Dataset loaded: {grape_dataset.__len__()} images and labels\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader is ready with 20 batches\n"
     ]
    }
   ],
   "source": [
    "#DATALOADER\n",
    "batch_size = 1\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))  \n",
    "\n",
    "train_loader = DataLoader(grape_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f'DataLoader is ready with {len(train_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "def plot_bboxes(image_tensor, bboxes_tensor):\n",
    "    \n",
    "    image = image_tensor.permute(1,2,0)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    for bbox in bboxes_tensor:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        rect = plt.Rectangle((xmin, ymin), width, height, fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_loss(outputs, verbose=False):\n",
    "    loss_classifier = outputs['loss_classifier']\n",
    "    loss_box_reg = outputs['loss_box_reg']\n",
    "    loss_objectness = outputs['loss_objectness']\n",
    "    loss_rpn_box_reg = outputs['loss_rpn_box_reg']\n",
    "\n",
    "    # Add all the losses together\n",
    "    total_loss = loss_classifier + loss_box_reg + loss_objectness + loss_rpn_box_reg\n",
    "\n",
    "    if verbose:\n",
    "        print(f'classifier loss: {loss_classifier:.4f}   box reg loss: {loss_box_reg:.4f}   objectness loss: {loss_objectness:.4f}   rpn loss: {loss_rpn_box_reg:.4f}   total loss: {total_loss:.4f}')\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, num_epochs=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    rpn_loss_history = []\n",
    "    box_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            \n",
    "            images = list(image.to(torch.float32) for image in images)\n",
    "            targets = [t for t in targets]\n",
    "            \n",
    "            no_bbox = False\n",
    "            for t in targets:\n",
    "                if t['boxes'].shape[0] == 0:\n",
    "                    no_bbox = True\n",
    "\n",
    "            if no_bbox:\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = calculate_loss(loss_dict, verbose=True)\n",
    "            \n",
    "            # if len(rpn_loss_history) > 3 and (abs(rpn_loss_history[-1] - loss_dict['loss_rpn_box_reg']) > 2 or abs(box_loss_history[-1] - loss_dict['loss_box_reg']) > 200):\n",
    "            # if len(rpn_loss_history) > 3 and (abs(box_loss_history[-1] - loss_dict['loss_box_reg']) > 200):\n",
    "            #     print('Bad loss, discarding..')\n",
    "            #     continue\n",
    "\n",
    "            if loss > 700:\n",
    "                print(f'Bad loss, discarding..')\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images[0].size(0)\n",
    "\n",
    "            rpn_loss_history.append(loss_dict['loss_rpn_box_reg'])\n",
    "            box_loss_history.append(loss_dict['loss_box_reg'])\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Average epoch loss: {epoch_loss:.4f}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier loss: 0.0000   box reg loss: 1137.4263   objectness loss: 9.5963   rpn loss: 61.4256   total loss: 1208.4481\n",
      "Bad loss, discarding..\n",
      "classifier loss: 0.0000   box reg loss: 204.2041   objectness loss: 4.6754   rpn loss: 0.7846   total loss: 209.6641\n",
      "classifier loss: 0.0000   box reg loss: 885.8781   objectness loss: 0.6718   rpn loss: 0.0655   total loss: 886.6154\n",
      "Bad loss, discarding..\n",
      "classifier loss: 0.0000   box reg loss: 377.1791   objectness loss: 1.4329   rpn loss: 0.0839   total loss: 378.6960\n",
      "classifier loss: 0.0000   box reg loss: 1206.8744   objectness loss: 9.4839   rpn loss: 50.6223   total loss: 1266.9805\n",
      "Bad loss, discarding..\n",
      "classifier loss: 0.0000   box reg loss: 318.1935   objectness loss: 0.6615   rpn loss: 0.0290   total loss: 318.8840\n",
      "classifier loss: 0.0000   box reg loss: 471.6931   objectness loss: 0.6454   rpn loss: 0.0472   total loss: 472.3857\n",
      "classifier loss: 0.0000   box reg loss: 274.0472   objectness loss: 0.8044   rpn loss: 0.0524   total loss: 274.9041\n",
      "classifier loss: 0.0000   box reg loss: 388.2845   objectness loss: 0.5558   rpn loss: 0.0499   total loss: 388.8901\n",
      "classifier loss: 0.0000   box reg loss: 415.0468   objectness loss: 1.9203   rpn loss: 0.7358   total loss: 417.7030\n",
      "classifier loss: 0.0000   box reg loss: 268.8177   objectness loss: 0.6109   rpn loss: 0.4987   total loss: 269.9273\n",
      "classifier loss: 0.0000   box reg loss: 468.3103   objectness loss: 0.7641   rpn loss: 0.1647   total loss: 469.2392\n",
      "classifier loss: 0.0000   box reg loss: 523.0992   objectness loss: 7.5646   rpn loss: 42.4664   total loss: 573.1302\n",
      "classifier loss: 0.0000   box reg loss: 648.5583   objectness loss: 0.4319   rpn loss: 0.1309   total loss: 649.1211\n",
      "classifier loss: 0.0000   box reg loss: 594.6940   objectness loss: 7.2820   rpn loss: 32.2634   total loss: 634.2394\n",
      "classifier loss: 0.0000   box reg loss: 213.1166   objectness loss: 0.5871   rpn loss: 0.0697   total loss: 213.7734\n",
      "classifier loss: 0.0000   box reg loss: 427.9399   objectness loss: 1.4717   rpn loss: 0.1481   total loss: 429.5597\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models.detection.faster_rcnn\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.optim as optim\n",
    "import torchvision.models.detection.roi_heads\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "num_classes = 1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(params, lr=1e-6)\n",
    "\n",
    "train(model, train_loader, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'grapejuice_model1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "model.load_state_dict(torch.load('grapejuice_model1.pth'))\n",
    "model.eval()\n",
    "\n",
    "input_image = Image.open('/Users/adibalaji/Desktop/grape_juice/stem_detection/grape_stem_data/grape_dataset/img/CFR_1626.jpg')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "input_image = transform(input_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_image)\n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
